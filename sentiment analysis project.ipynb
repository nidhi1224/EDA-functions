{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a884072a-cf6c-4fe1-a108-136d5c62954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346.0</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797.0</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044.0</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272.0</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment  \\\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3  01082688c6                                        happy bday!  positive   \n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
       "\n",
       "  Time of Tweet Age of User      Country  Population -2020  Land Area (Km²)  \\\n",
       "0       morning        0-20  Afghanistan        38928346.0         652860.0   \n",
       "1          noon       21-30      Albania         2877797.0          27400.0   \n",
       "2         night       31-45      Algeria        43851044.0        2381740.0   \n",
       "3       morning       46-60      Andorra           77265.0            470.0   \n",
       "4          noon       60-70       Angola        32866272.0        1246700.0   \n",
       "\n",
       "   Density (P/Km²)  \n",
       "0             60.0  \n",
       "1            105.0  \n",
       "2             18.0  \n",
       "3            164.0  \n",
       "4             26.0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the CSV file with specified encoding\n",
    "file_path = r\"C:\\Users\\Home\\Downloads\\test (3).csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Convert all values in the text column to strings and handle missing values\n",
    "df['text'] = df['text'].astype(str).fillna('')\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16d863f2-a246-4b2e-b754-9a3e946ceb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb896b5c-7609-479c-9f84-9d8bfa5aa496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0bcde07f-552a-4735-a306-41bad28380e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 last session day httptwitpiccom67ezh\n",
       "1    shanghai also really exciting precisely skyscr...\n",
       "2    recession hit veronique branquinho quit compan...\n",
       "3                                           happy bday\n",
       "4                             httptwitpiccom4w75p like\n",
       "5                             thats great weee visitor\n",
       "6                              think everyone hate lol\n",
       "7    soooooo wish could im school myspace completel...\n",
       "8                          within short time last clue\n",
       "9    get day alright havent done anything yet leavi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5354878-5a78-41d5-9ff4-42e6c2cf0ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>last session day httptwitpiccom67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346.0</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>shanghai also really exciting precisely skyscr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797.0</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>recession hit veronique branquinho quit compan...</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044.0</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>httptwitpiccom4w75p like</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272.0</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>726e501993</td>\n",
       "      <td>thats great weee visitor</td>\n",
       "      <td>positive</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>97929.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>261932614e</td>\n",
       "      <td>think everyone hate lol</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>45195774.0</td>\n",
       "      <td>2736690.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>afa11da83f</td>\n",
       "      <td>soooooo wish could im school myspace completel...</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>2963243.0</td>\n",
       "      <td>28470.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e64208b4ef</td>\n",
       "      <td>within short time last clue</td>\n",
       "      <td>neutral</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Australia</td>\n",
       "      <td>25499884.0</td>\n",
       "      <td>7682300.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37bcad24ca</td>\n",
       "      <td>get day alright havent done anything yet leavi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Austria</td>\n",
       "      <td>9006398.0</td>\n",
       "      <td>82400.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment  \\\n",
       "0  f87dea47db               last session day httptwitpiccom67ezh   neutral   \n",
       "1  96d74cb729  shanghai also really exciting precisely skyscr...  positive   \n",
       "2  eee518ae67  recession hit veronique branquinho quit compan...  negative   \n",
       "3  01082688c6                                         happy bday  positive   \n",
       "4  33987a8ee5                           httptwitpiccom4w75p like  positive   \n",
       "5  726e501993                           thats great weee visitor  positive   \n",
       "6  261932614e                            think everyone hate lol  negative   \n",
       "7  afa11da83f  soooooo wish could im school myspace completel...  negative   \n",
       "8  e64208b4ef                        within short time last clue   neutral   \n",
       "9  37bcad24ca  get day alright havent done anything yet leavi...   neutral   \n",
       "\n",
       "  Time of Tweet Age of User              Country  Population -2020  \\\n",
       "0       morning        0-20          Afghanistan        38928346.0   \n",
       "1          noon       21-30              Albania         2877797.0   \n",
       "2         night       31-45              Algeria        43851044.0   \n",
       "3       morning       46-60              Andorra           77265.0   \n",
       "4          noon       60-70               Angola        32866272.0   \n",
       "5         night      70-100  Antigua and Barbuda           97929.0   \n",
       "6       morning        0-20            Argentina        45195774.0   \n",
       "7          noon       21-30              Armenia         2963243.0   \n",
       "8         night       31-45            Australia        25499884.0   \n",
       "9       morning       46-60              Austria         9006398.0   \n",
       "\n",
       "   Land Area (Km²)  Density (P/Km²)  \n",
       "0         652860.0             60.0  \n",
       "1          27400.0            105.0  \n",
       "2        2381740.0             18.0  \n",
       "3            470.0            164.0  \n",
       "4        1246700.0             26.0  \n",
       "5            440.0            223.0  \n",
       "6        2736690.0             17.0  \n",
       "7          28470.0            104.0  \n",
       "8        7682300.0              3.0  \n",
       "9          82400.0            109.0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c658e95d-40ee-4f83-8763-c5186193f53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>final_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>last session day httptwitpiccom67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346.0</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>shanghai also really exciting precisely skyscr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797.0</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compou...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>recession hit veronique branquinho quit compan...</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044.0</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>{'neg': 0.541, 'neu': 0.459, 'pos': 0.0, 'comp...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.213, 'pos': 0.787, 'comp...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>httptwitpiccom4w75p like</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272.0</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'comp...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment  \\\n",
       "0  f87dea47db               last session day httptwitpiccom67ezh   neutral   \n",
       "1  96d74cb729  shanghai also really exciting precisely skyscr...  positive   \n",
       "2  eee518ae67  recession hit veronique branquinho quit compan...  negative   \n",
       "3  01082688c6                                         happy bday  positive   \n",
       "4  33987a8ee5                           httptwitpiccom4w75p like  positive   \n",
       "\n",
       "  Time of Tweet Age of User      Country  Population -2020  Land Area (Km²)  \\\n",
       "0       morning        0-20  Afghanistan        38928346.0         652860.0   \n",
       "1          noon       21-30      Albania         2877797.0          27400.0   \n",
       "2         night       31-45      Algeria        43851044.0        2381740.0   \n",
       "3       morning       46-60      Andorra           77265.0            470.0   \n",
       "4          noon       60-70       Angola        32866272.0        1246700.0   \n",
       "\n",
       "   Density (P/Km²)                                   sentiment_scores  \\\n",
       "0             60.0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "1            105.0  {'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compou...   \n",
       "2             18.0  {'neg': 0.541, 'neu': 0.459, 'pos': 0.0, 'comp...   \n",
       "3            164.0  {'neg': 0.0, 'neu': 0.213, 'pos': 0.787, 'comp...   \n",
       "4             26.0  {'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'comp...   \n",
       "\n",
       "  final_sentiment  \n",
       "0         neutral  \n",
       "1        positive  \n",
       "2        negative  \n",
       "3        positive  \n",
       "4        positive  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the VADER sentiment intensity analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get the sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['sentiment_scores'] = df['text'].apply(get_sentiment_score)\n",
    "\n",
    "# Function to determine final sentiment based on compound score\n",
    "def final_sentiment(compound):\n",
    "    if compound >= 0.10:\n",
    "        return 'positive'\n",
    "    elif compound <= -0.10:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply the final sentiment function to the DataFrame\n",
    "df['final_sentiment'] = df['sentiment_scores'].apply(lambda x: final_sentiment(x['compound']))\n",
    "\n",
    "# Display the DataFrame with sentiment scores and final sentiment\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4ccc40e-8bfb-4452-87bc-24ac2c04f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VADER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.57      0.63      1001\n",
      "     neutral       0.69      0.51      0.59      1430\n",
      "    positive       0.58      0.87      0.69      1103\n",
      "\n",
      "    accuracy                           0.64      3534\n",
      "   macro avg       0.66      0.65      0.64      3534\n",
      "weighted avg       0.66      0.64      0.63      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    " \n",
    "# Ensure columns contain only strings\n",
    "df['sentiment'] = df['sentiment'].astype(str)\n",
    "df['final_sentiment'] = df['final_sentiment'].astype(str)\n",
    " \n",
    "# Print the classification report for VADER\n",
    "print(\"\\nVADER Classification Report:\")\n",
    "print(classification_report(df['sentiment'], df['final_sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "29b1f03a-5894-4392-b1cb-77434e982997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Vectorize text data using Word2Vec\n",
    "def vectorize_text(tokens, model):\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in model.wv.key_to_index:\n",
    "            vector += model.wv[token]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vector /= count\n",
    "    return vector\n",
    "\n",
    "df['vector'] = df['tokens'].apply(lambda x: vectorize_text(x, word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "14acd6fd-10c3-4601-a1b9-d6f203d04b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-0.013849512324668467, 0.014759456273168325, ...\n",
       "1       [-0.004895619834618022, 0.0033858509462637207,...\n",
       "2       [-6.5052615744727e-05, 0.002306538534217647, -...\n",
       "3       [0.0029051120509393513, 0.010322751011699438, ...\n",
       "4       [-0.015536025632172823, 0.023985980813449714, ...\n",
       "                              ...                        \n",
       "3529    [-0.009542831918224692, 0.028368771076202393, ...\n",
       "3530    [-0.004603378800675273, 0.0075027830355490245,...\n",
       "3531    [-0.00375081836245954, 0.01452718215296045, 0....\n",
       "3532    [-0.003884290374116972, 0.012978789542103186, ...\n",
       "3533    [-0.0017759899298350017, 0.001633494236102706,...\n",
       "Name: vector, Length: 3534, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af106aef-3ce9-414b-a9c1-aa6424b88f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning models\n",
    "X = np.array(df['vector'].tolist())\n",
    "y = df['sentiment']\n",
    "\n",
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2478471b-5305-48b8-94dc-a4b6a39f8477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       207\n",
      "     neutral       0.42      0.95      0.58       286\n",
      "    positive       0.50      0.12      0.19       214\n",
      "\n",
      "    accuracy                           0.42       707\n",
      "   macro avg       0.31      0.36      0.26       707\n",
      "weighted avg       0.32      0.42      0.29       707\n",
      "\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.03      0.06       207\n",
      "     neutral       0.45      0.95      0.61       286\n",
      "    positive       0.73      0.30      0.42       214\n",
      "\n",
      "    accuracy                           0.49       707\n",
      "   macro avg       0.65      0.43      0.37       707\n",
      "weighted avg       0.63      0.49      0.39       707\n",
      "\n",
      "\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.32      0.45       207\n",
      "     neutral       0.51      0.81      0.62       286\n",
      "    positive       0.71      0.54      0.61       214\n",
      "\n",
      "    accuracy                           0.59       707\n",
      "   macro avg       0.66      0.56      0.56       707\n",
      "weighted avg       0.64      0.59      0.57       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Home\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Home\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Support Vector Machine\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Naive Bayes\n",
    "# Naive Bayes typically works with TF-IDF vectors rather than Word2Vec\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_nb = nb.predict(X_test_tfidf)\n",
    "\n",
    "# Print classification reports\n",
    "print(\"\\nLogistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_log_reg, target_names=le.classes_))\n",
    "\n",
    "print(\"\\nSVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=le.classes_))\n",
    "\n",
    "print(\"\\nNaive Bayes Classification Report:\")\n",
    "print(classification_report(y_test_tfidf, y_pred_nb, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "27e0828c-1f39-4245-976e-e53beafe0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I really really like the song Love Story by Taylor Swift\"\n",
    "scores = sid.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "00981211-f103-409d-9b22-15fc93914018",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is waste of product\"\n",
    "scores = sid.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e63db5ed-339a-41c2-b376-dd4e93b79565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.351, 'pos': 0.649, 'compound': 0.5719}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"bootcamp is excellent\"\n",
    "scores = sid.polarity_scores(text)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1b3df4b1-5993-42fb-978f-ce073acdf398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"bootcamp is not bad \"\n",
    "scores = sid.polarity_scores(text)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0c5ab97c-607e-472d-acb9-4a70e1e26059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.445, 'neu': 0.555, 'pos': 0.0, 'compound': -0.3412}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"bootcamp is not good \"\n",
    "scores = sid.polarity_scores(text)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c4081874-d22d-4cc7-82de-53493fad25c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.308, 'neu': 0.419, 'pos': 0.273, 'compound': -0.0865}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"bootcamp is somewhat good somewhat bad\"\n",
    "scores = sid.polarity_scores(text)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3728fead-c954-4f63-972a-769ded16a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"bootcamp is too good.I like bootcamp a lot\"\n",
    "scores = sid.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cd7f1c79-6224-49d0-8df0-8788b4646af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'compound': -0.5423}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"bootcamp is too bad\"\n",
    "scores = sid.polarity_scores(text)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9a3094d0-b39d-476b-9993-6d3f2b3dd102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"bootcamp is too good\"\n",
    "scores = sid.polarity_scores(text)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "57c84d06-d42c-4579-b15a-d39ed41beec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "62c1c633-c3a1-48ab-af41-1ac2699c453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_sentiment(compound):\n",
    "    if compound >= 0.10:\n",
    "        return 'positive'\n",
    "    elif compound <= -0.10:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
