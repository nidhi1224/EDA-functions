{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93b410fc-c42d-4e5a-b465-580d1502714a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7b2f240-3c3e-4abc-9dc6-6adf645db4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:\n",
      "['The children are playing in the garden.', 'She enjoys running every morning.', 'He has been a runner for ten years.', 'Dogs are loyal animals.', 'She fishes every weekend at the lake.', 'The fisherman has fished in this river for decades.', 'This product is better than the other one.', 'She bought a new fishing rod.']\n",
      "\n",
      "Documents:\n",
      "Document 1: The children are playing in the garden.\n",
      "Document 2: She enjoys running every morning.\n",
      "Document 3: He has been a runner for ten years.\n",
      "Document 4: Dogs are loyal animals.\n",
      "Document 5: She fishes every weekend at the lake.\n",
      "Document 6: The fisherman has fished in this river for decades.\n",
      "Document 7: This product is better than the other one.\n",
      "Document 8: She bought a new fishing rod.\n",
      "\n",
      "Tokens:\n",
      "Tokens for Document 1: ['the', 'children', 'are', 'playing', 'in', 'the', 'garden']\n",
      "Tokens for Document 2: ['she', 'enjoys', 'running', 'every', 'morning']\n",
      "Tokens for Document 3: ['he', 'has', 'been', 'a', 'runner', 'for', 'ten', 'years']\n",
      "Tokens for Document 4: ['dogs', 'are', 'loyal', 'animals']\n",
      "Tokens for Document 5: ['she', 'fishes', 'every', 'weekend', 'at', 'the', 'lake']\n",
      "Tokens for Document 6: ['the', 'fisherman', 'has', 'fished', 'in', 'this', 'river', 'for', 'decades']\n",
      "Tokens for Document 7: ['this', 'product', 'is', 'better', 'than', 'the', 'other', 'one']\n",
      "Tokens for Document 8: ['she', 'bought', 'a', 'new', 'fishing', 'rod']\n",
      "\n",
      "Vocabulary:\n",
      "{'enjoys', 'ten', 'he', 'a', 'fishing', 'every', 'other', 'been', 'new', 'playing', 'weekend', 'runner', 'lake', 'years', 'for', 'animals', 'is', 'she', 'running', 'children', 'decades', 'product', 'this', 'fisherman', 'morning', 'in', 'bought', 'than', 'at', 'rod', 'garden', 'the', 'has', 'dogs', 'fishes', 'river', 'fished', 'one', 'are', 'loyal', 'better'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"The children are playing in the garden.\",\n",
    "    \"She enjoys running every morning.\",\n",
    "    \"He has been a runner for ten years.\",\n",
    "    \"Dogs are loyal animals.\",\n",
    "    \"She fishes every weekend at the lake.\",\n",
    "    \"The fisherman has fished in this river for decades.\",\n",
    "    \"This product is better than the other one.\",\n",
    "    \"She bought a new fishing rod.\"\n",
    "]\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize each document in the corpus\n",
    "tokenized_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = set(word for tokens in tokenized_corpus for word in tokens)\n",
    "\n",
    "# Print the results\n",
    "print(\"Corpus:\")\n",
    "print(corpus)\n",
    "print(\"\\nDocuments:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "print(\"\\nTokens:\")\n",
    "for i, tokens in enumerate(tokenized_corpus):\n",
    "    print(f\"Tokens for Document {i+1}: {tokens}\")\n",
    "print(\"\\nVocabulary:\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30bb507d-c7ed-4bcd-b3d4-29cf6602ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    " \n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58fee002-5c25-450d-8d6c-b00937b5cc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Stop Words:\n",
      "{\"she's\", 'won', \"aren't\", 'he', 'we', 'any', 'such', 'own', \"hadn't\", 'of', \"mustn't\", 'during', 'didn', 'what', 'for', 'hers', 'and', 'once', 'is', 'ma', 'where', 'off', 'do', 'theirs', 'themselves', 'below', 'ain', 'can', 'y', 'from', 'to', 'yourselves', 'only', 'further', 'at', 'why', \"couldn't\", 'has', 'aren', 'll', 'mustn', 'an', 'now', \"hasn't\", 'up', 'are', 'yours', 'here', 're', 'with', 'each', 'yourself', 'some', 'ours', 'over', 'there', 'both', 'same', 'isn', 'but', 'myself', 'himself', 'how', \"don't\", 'does', \"haven't\", 'had', 'than', 'd', 's', 't', 'you', 'his', 'who', 'am', 'out', 'needn', 'just', 'don', \"won't\", \"didn't\", 'all', 'nor', \"shan't\", 'which', \"that'll\", 'too', 'be', \"you'll\", 'm', \"you've\", 'not', 'it', 'if', 'her', 'weren', 'other', 'then', 'have', 'hadn', 'couldn', 'them', 'shan', 'about', 'very', \"needn't\", 'on', 'down', 'this', 'between', 'under', \"you're\", 'when', 'shouldn', 'o', 'as', 'the', 'no', 'whom', \"wasn't\", \"isn't\", 'before', 'our', 'wouldn', 'mightn', 'until', 'after', \"wouldn't\", 'him', 'they', 'doing', 'through', 'above', 'itself', 'against', 'by', 'herself', 've', 'a', 'those', 'so', 'been', \"should've\", 'wasn', \"you'd\", 'or', 'was', 'she', \"mightn't\", 'these', 'were', \"shouldn't\", 'more', \"weren't\", 'again', 'most', 'doesn', 'few', 'haven', 'in', 'while', 'ourselves', 'that', 'because', \"doesn't\", 'your', 'into', 'having', 'hasn', 'me', 'i', 'being', 'will', 'did', 'my', 'should', \"it's\", 'its', 'their'}\n"
     ]
    }
   ],
   "source": [
    "# Get the list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print all stop words\n",
    "print(\"English Stop Words:\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7de53ac-aec9-413f-ade9-82e7289c4bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:\n",
      "['The children are playing in the garden.', 'She enjoys running every morning.', 'He has been a runner for ten years.', 'Dogs are loyal animals.', 'She fishes every weekend at the lake.', 'The fisherman has fished in this river for decades.', 'This product is better than the other one.', 'She bought a new fishing rod.']\n",
      "\n",
      "Documents:\n",
      "Document 1: The children are playing in the garden.\n",
      "Document 2: She enjoys running every morning.\n",
      "Document 3: He has been a runner for ten years.\n",
      "Document 4: Dogs are loyal animals.\n",
      "Document 5: She fishes every weekend at the lake.\n",
      "Document 6: The fisherman has fished in this river for decades.\n",
      "Document 7: This product is better than the other one.\n",
      "Document 8: She bought a new fishing rod.\n",
      "\n",
      "Tokens:\n",
      "Tokens for Document 1: ['children', 'playing', 'garden']\n",
      "Tokens for Document 2: ['enjoys', 'running', 'every', 'morning']\n",
      "Tokens for Document 3: ['runner', 'ten', 'years']\n",
      "Tokens for Document 4: ['dogs', 'loyal', 'animals']\n",
      "Tokens for Document 5: ['fishes', 'every', 'weekend', 'lake']\n",
      "Tokens for Document 6: ['fisherman', 'fished', 'river', 'decades']\n",
      "Tokens for Document 7: ['product', 'better', 'one']\n",
      "Tokens for Document 8: ['bought', 'new', 'fishing', 'rod']\n",
      "\n",
      "Vocabulary:\n",
      "{'enjoys', 'ten', 'fishing', 'every', 'new', 'playing', 'weekend', 'runner', 'lake', 'years', 'animals', 'running', 'children', 'decades', 'product', 'fisherman', 'morning', 'bought', 'rod', 'garden', 'river', 'fishes', 'dogs', 'fished', 'one', 'loyal', 'better'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"The children are playing in the garden.\",\n",
    "    \"She enjoys running every morning.\",\n",
    "    \"He has been a runner for ten years.\",\n",
    "    \"Dogs are loyal animals.\",\n",
    "    \"She fishes every weekend at the lake.\",\n",
    "    \"The fisherman has fished in this river for decades.\",\n",
    "    \"This product is better than the other one.\",\n",
    "    \"She bought a new fishing rod.\"\n",
    "]\n",
    " \n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens\n",
    " \n",
    "# Tokenize each document in the corpus\n",
    "tokenized_corpus = [preprocess_text(doc) for doc in corpus]\n",
    " \n",
    "# Build vocabulary\n",
    "vocabulary = set(word for tokens in tokenized_corpus for word in tokens)\n",
    " \n",
    "# Print the results\n",
    "print(\"Corpus:\")\n",
    "print(corpus)\n",
    "print(\"\\nDocuments:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "print(\"\\nTokens:\")\n",
    "for i, tokens in enumerate(tokenized_corpus):\n",
    "    print(f\"Tokens for Document {i+1}: {tokens}\")\n",
    "print(\"\\nVocabulary:\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61cfd666-90c7-4852-8e52-4c53cab39387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    " \n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9bc33e9b-adbe-477f-aa1d-6b16ea4b6a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: The children are playing in the garden.\n",
      "Filtered Tokens: ['children', 'playing', 'garden']\n",
      "Stemmed Tokens: ['children', 'play', 'garden']\n",
      "Lemmatized Tokens: ['child', 'play', 'garden']\n",
      "--------------------------------------------------\n",
      "Original Document: She enjoys running every morning.\n",
      "Filtered Tokens: ['enjoys', 'running', 'every', 'morning']\n",
      "Stemmed Tokens: ['enjoy', 'run', 'everi', 'morn']\n",
      "Lemmatized Tokens: ['enjoys', 'run', 'every', 'morning']\n",
      "--------------------------------------------------\n",
      "Original Document: He has been a runner for ten years.\n",
      "Filtered Tokens: ['runner', 'ten', 'years']\n",
      "Stemmed Tokens: ['runner', 'ten', 'year']\n",
      "Lemmatized Tokens: ['runner', 'ten', 'year']\n",
      "--------------------------------------------------\n",
      "Original Document: Dogs are loyal animals.\n",
      "Filtered Tokens: ['dogs', 'loyal', 'animals']\n",
      "Stemmed Tokens: ['dog', 'loyal', 'anim']\n",
      "Lemmatized Tokens: ['dog', 'loyal', 'animal']\n",
      "--------------------------------------------------\n",
      "Original Document: She fishes every weekend at the lake.\n",
      "Filtered Tokens: ['fishes', 'every', 'weekend', 'lake']\n",
      "Stemmed Tokens: ['fish', 'everi', 'weekend', 'lake']\n",
      "Lemmatized Tokens: ['fish', 'every', 'weekend', 'lake']\n",
      "--------------------------------------------------\n",
      "Original Document: The fisherman has fished in this river for decades.\n",
      "Filtered Tokens: ['fisherman', 'fished', 'river', 'decades']\n",
      "Stemmed Tokens: ['fisherman', 'fish', 'river', 'decad']\n",
      "Lemmatized Tokens: ['fisherman', 'fish', 'river', 'decade']\n",
      "--------------------------------------------------\n",
      "Original Document: This product is better than the other one.\n",
      "Filtered Tokens: ['product', 'better', 'one']\n",
      "Stemmed Tokens: ['product', 'better', 'one']\n",
      "Lemmatized Tokens: ['product', 'well', 'one']\n",
      "--------------------------------------------------\n",
      "Original Document: She bought a new fishing rod.\n",
      "Filtered Tokens: ['bought', 'new', 'fishing', 'rod']\n",
      "Stemmed Tokens: ['bought', 'new', 'fish', 'rod']\n",
      "Lemmatized Tokens: ['bought', 'new', 'fishing', 'rod']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the Porter Stemmer and WordNet Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"The children are playing in the garden.\",\n",
    "    \"She enjoys running every morning.\",\n",
    "    \"He has been a runner for ten years.\",\n",
    "    \"Dogs are loyal animals.\",\n",
    "    \"She fishes every weekend at the lake.\",\n",
    "    \"The fisherman has fished in this river for decades.\",\n",
    "    \"This product is better than the other one.\",\n",
    "    \"She bought a new fishing rod.\"\n",
    "]\n",
    "\n",
    "# Function to get the part of speech tag for lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Function to preprocess text and perform stemming and lemmatization\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_tokens]\n",
    "    return filtered_tokens, stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "# Process each document in the corpus\n",
    "for document in corpus:\n",
    "    filtered_tokens, stemmed_tokens, lemmatized_tokens = preprocess_text(document)\n",
    "    print(f\"Original Document: {document}\")\n",
    "    print(f\"Filtered Tokens: {filtered_tokens}\")\n",
    "    print(f\"Stemmed Tokens: {stemmed_tokens}\")\n",
    "    print(f\"Lemmatized Tokens: {lemmatized_tokens}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
